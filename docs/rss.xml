<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Magnus</title><link>https://Liang95869417.github.io/data-engineering</link><description>Sharing tech details about data engineering</description><copyright>Magnus</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://avatars.githubusercontent.com/u/118542956?v=4</url><title>avatar</title><link>https://Liang95869417.github.io/data-engineering</link></image><lastBuildDate>Fri, 05 Jul 2024 13:37:23 +0000</lastBuildDate><managingEditor>Magnus</managingEditor><ttl>60</ttl><webMaster>Magnus</webMaster><item><title>Code Anatomy - Scheduling Data Pipelines with Apache Airflow</title><link>https://Liang95869417.github.io/data-engineering/post/Code%20Anatomy%20-%20Scheduling%20Data%20Pipelines%20with%20Apache%20Airflow.html</link><description>Scheduling and monitoring data pipelines are key responsibilities in data engineering, ensuring that data flows smoothly and efficiently through various systems. Apache Airflow stands out as a powerful tool for orchestrating these workflows programmatically. However, crafting efficient and effective Airflow DAGs (Directed Acyclic Graphs) can be a bit complex. In this post, we’ll break down an Airflow DAG example to make its structure and functionality clear and understandable.&#13;
&#13;
# Basic Airflow DAG Example&#13;
Let’s start with a simple Airflow DAG that schedules a data pipeline. Here’s the initial code:&#13;
```&#13;
from airflow import DAG&#13;
from airflow.operators.empty import EmptyOperator&#13;
from airflow.operators.python import PythonOperator&#13;
from datetime import datetime, timedelta&#13;
&#13;
# Default arguments&#13;
default_args = {&#13;
    'owner': 'airflow',&#13;
    'depends_on_past': False,&#13;
    'start_date': datetime(2024, 1, 1),&#13;
    'email_on_failure': False,&#13;
    'email_on_retry': False,&#13;
    'retries': 1,&#13;
    'retry_delay': timedelta(minutes=5),&#13;
}&#13;
&#13;
# Initialize the DAG&#13;
dag = DAG(&#13;
    'scheduling_data_pipeline',&#13;
    default_args=default_args,&#13;
    description='An example DAG for scheduling data pipeline',&#13;
    schedule=timedelta(days=1),&#13;
)&#13;
&#13;
# Define the tasks&#13;
start_task = EmptyOperator(task_id='start_task', dag=dag)&#13;
&#13;
def print_hello():&#13;
    return 'Hello world!'&#13;
&#13;
hello_task = PythonOperator(&#13;
    task_id='hello_task',&#13;
    python_callable=print_hello,&#13;
    dag=dag,&#13;
)&#13;
&#13;
end_task = EmptyOperator(task_id='end_task', dag=dag)&#13;
&#13;
# Set task dependencies&#13;
start_task &gt;&gt; hello_task &gt;&gt; end_task&#13;
```&#13;
# Understanding the Code&#13;
## 1. Default Arguments&#13;
```&#13;
default_args = {&#13;
    'owner': 'airflow',&#13;
    'depends_on_past': False,&#13;
    'start_date': datetime(2024, 1, 1),&#13;
    'email_on_failure': False,&#13;
    'email_on_retry': False,&#13;
    'retries': 1,&#13;
    'retry_delay': timedelta(minutes=5),&#13;
}&#13;
```&#13;
- owner: The owner of the DAG.&#13;
- depends_on_past: If set to True, tasks depend on the success of the previous task instance.&#13;
- start_date: The date and time when the DAG should start running.&#13;
- email_on_failure: Whether to send an email when a task fails.&#13;
- email_on_retry: Whether to send an email when a task is retried.&#13;
- retries: Number of times to retry a failed task.&#13;
- retry_delay: Time delay between retries.&#13;
&#13;
## 2. Initializing the DAG&#13;
```&#13;
dag = DAG(&#13;
    'scheduling_data_pipeline',&#13;
    default_args=default_args,&#13;
    description='An example DAG for scheduling data pipeline',&#13;
    schedule=timedelta(days=1),&#13;
)&#13;
```&#13;
- DAG: Defines the DAG with a unique name and the default arguments.&#13;
- schedule: The interval at which the DAG should run. Here, it's set to run daily.&#13;
&#13;
## 3. Defining the Tasks&#13;
```&#13;
start_task = EmptyOperator(task_id='start_task', dag=dag)&#13;
&#13;
def print_hello():&#13;
    return 'Hello world!'&#13;
&#13;
hello_task = PythonOperator(&#13;
    task_id='hello_task',&#13;
    python_callable=print_hello,&#13;
    dag=dag,&#13;
)&#13;
&#13;
end_task = EmptyOperator(task_id='end_task', dag=dag)&#13;
```&#13;
- EmptyOperator: A no-op operator that does nothing. Used here to signify the start and end of the pipeline.&#13;
- PythonOperator: Executes a Python callable. In this case, it calls the print_hello function.&#13;
&#13;
## 4. Setting Task Dependencies&#13;
```&#13;
start_task &gt;&gt; hello_task &gt;&gt; end_task&#13;
```&#13;
- This sets the order of task execution: start_task runs first, followed by hello_task, and finally end_task.。</description><guid isPermaLink="true">https://Liang95869417.github.io/data-engineering/post/Code%20Anatomy%20-%20Scheduling%20Data%20Pipelines%20with%20Apache%20Airflow.html</guid><pubDate>Fri, 05 Jul 2024 13:05:48 +0000</pubDate></item></channel></rss>